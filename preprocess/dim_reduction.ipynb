{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4da9766-8094-4aa1-80b3-a5dbf6228366",
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports ###\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "# add paths to access shared code\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../scripts/\")\n",
    "\n",
    "# import library implementing models\n",
    "import neuroprob as nprb\n",
    "from neuroprob import utils\n",
    "\n",
    "# import utility code for model building/training/loading\n",
    "import lib\n",
    "import HDC\n",
    "\n",
    "# get GPU device if available\n",
    "gpu_dev = 0\n",
    "dev = utils.pytorch.get_device(gpu=gpu_dev)\n",
    "\n",
    "# use custom plotting settings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.style.use(['paper.mplstyle'])\n",
    "\n",
    "# useful colormap\n",
    "white = '#ffffff'\n",
    "red = '#ff0000'\n",
    "blue = '#0000ff'\n",
    "weight_map = utils.plot.make_cmap([blue, white, red], 'weight_map')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18ffb4e-dc38-4291-8949-7626a6438f4c",
   "metadata": {},
   "source": [
    "Load model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37ff4b7b-e5a2-4fef-a2ea-fa59bd77ea2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../example/HDC12-120806wake_Uqd3_H0_svgp64_X[hd-w-s-pos-t]_Z[]_50K37_0d0_5f-1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m dataset_tuple \u001b[38;5;241m=\u001b[39m HDC\u001b[38;5;241m.\u001b[39mget_dataset(session_id, phase, bin_size, single_spikes, path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../checkpoint/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# model and train/test split based on which validation fold is given by cv_run\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m modelfit, fit_set, validation_set \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_tuple\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mHDC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs_used\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mHDC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menc_used\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu_dev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjitter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\n\u001b[1;32m     31\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m rcov, units_used, tbin, resamples, rc_t, max_count, bin_size, metainfo, data_name \u001b[38;5;241m=\u001b[39m dataset_tuple\n",
      "File \u001b[0;32m~/UCM-interpret/preprocess/../scripts/lib/models.py:659\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(checkpoint_dir, m, dataset_tuple, inputs_used, enc_used, delay, cv_run, batch_info, gpu, trial_sizes, tensor_type, jitter, J)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;66;03m### load\u001b[39;00m\n\u001b[1;32m    658\u001b[0m model_name \u001b[38;5;241m=\u001b[39m gen_name(mdl_name, m, bin_size, max_count, delay, cv_run)\n\u001b[0;32m--> 659\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda:\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpu\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m full_model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_model\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m full_model, fit_set, validation_set\n",
      "File \u001b[0;32m~/py3_9env/lib/python3.9/site-packages/torch/serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    697\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 699\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    701\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/py3_9env/lib/python3.9/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/py3_9env/lib/python3.9/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../example/HDC12-120806wake_Uqd3_H0_svgp64_X[hd-w-s-pos-t]_Z[]_50K37_0d0_5f-1'"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = '../example/'\n",
    "session_id = '12-120806'\n",
    "phase = 'wake'\n",
    "\n",
    "single_spikes = False\n",
    "dt = 0.001\n",
    "\n",
    "ll_mode = 'Uqd3'  # stands for universal count model with exponential-quadratic expansion and C = 3\n",
    "filt_mode = ''\n",
    "map_mode = 'svgp64'  # a sparse variational GP mapping with 64 inducing points\n",
    "x_mode = 'hd-w-s-pos-t'  # observed covariates (behaviour)\n",
    "z_mode = ''  # latent covariates\n",
    "hist_len = 0\n",
    "folds = 5  # cross-validation folds\n",
    "delays = [0]\n",
    "model_info = (ll_mode, filt_mode, map_mode, x_mode, z_mode, hist_len, folds, delays)\n",
    "\n",
    "bin_size = 50  # ms\n",
    "cv_run = -1  # test set is last 1/5 of dataset time series\n",
    "delay = 0\n",
    "batch_size = 5000  # size of time segments of each batch in dataset below\n",
    "\n",
    "    \n",
    "# data\n",
    "dataset_tuple = HDC.get_dataset(session_id, phase, bin_size, single_spikes, path='../checkpoint/')\n",
    "\n",
    "# model and train/test split based on which validation fold is given by cv_run\n",
    "modelfit, fit_set, validation_set = lib.models.load_model(\n",
    "    checkpoint_dir, model_info, dataset_tuple, HDC.inputs_used, HDC.enc_used, \n",
    "    delay, cv_run, batch_size, gpu_dev, tensor_type=torch.float, jitter=1e-5\n",
    ")\n",
    "rcov, units_used, tbin, resamples, rc_t, max_count, bin_size, metainfo, data_name = dataset_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57045f3-acd0-4c79-8eb2-979e12e1a629",
   "metadata": {},
   "outputs": [],
   "source": [
    "hd_sweep = torch.linspace(0, 2*np.pi, 100)\n",
    "\n",
    "covariates = torch.cat([\n",
    "    hd_sweep[:, None],  # sweep over head direction\n",
    "    *[rcov[k].mean()*torch.ones((100, 1)) for k in range(1, len(rcov))], \n",
    "    # fill other behavioural covariates at value 0\n",
    "], dim=-1)[None, None, ...]  # (tr, neurons, steps, covariate dims)\n",
    "\n",
    "with torch.no_grad():\n",
    "    P_mc = lib.helper.compute_P(\n",
    "        modelfit, covariates, list(range(units_used)), MC=30, trials=1)  # predictive posterior\n",
    "P_mc = P_mc.cpu()  # count probabilities of shape (MC, neurons, steps, count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
